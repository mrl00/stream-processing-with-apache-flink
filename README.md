# Stream Processing with Apache Flink

Projeto de processamento de streams em tempo real utilizando Apache Flink e Apache Kafka, desenvolvido em Go. O sistema simula um ambiente bancÃ¡rio processando transaÃ§Ãµes financeiras, contas e clientes.

## ğŸ“‹ VisÃ£o Geral

Este projeto implementa uma arquitetura de processamento de streams que:

- **Produz dados** de contas, clientes e transaÃ§Ãµes para tÃ³picos Kafka usando produtores Go
- **Processa streams** em tempo real com Apache Flink
- **Gerencia dados** atravÃ©s de um cluster Kafka distribuÃ­do (3 brokers)
- **Suporta ambientes** local e Docker

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Producers     â”‚â”€â”€â”€â”€â–¶â”‚    Kafka     â”‚â”€â”€â”€â”€â–¶â”‚    Flink    â”‚
â”‚   (Go Apps)     â”‚     â”‚   Cluster    â”‚     â”‚  (SQL Jobs) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                         â”‚
     â”‚                         â”‚
     â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Datasets  â”‚         â”‚   Topics     â”‚
â”‚    (CSV)    â”‚         â”‚  (Streams)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes Principais

1. **Producers (Go)**: AplicaÃ§Ãµes que leem dados CSV e publicam em tÃ³picos Kafka
   - `account_producer`: Produz dados de contas
   - `customer_producer`: Produz dados de clientes
   - `transaction_producer`: Produz todas as transaÃ§Ãµes
   - `transaction_credit_producer`: Produz apenas transaÃ§Ãµes de crÃ©dito
   - `transaction_debit_producer`: Produz apenas transaÃ§Ãµes de dÃ©bito

2. **Apache Kafka**: Cluster com 3 brokers para alta disponibilidade
   - TÃ³picos: `accounts`, `customers`, `transactions`, `transactions.credits`, `transactions.debits`
   - ReplicaÃ§Ã£o: 3x
   - PartiÃ§Ãµes: 3

3. **Apache Flink**: Processamento de streams
   - JobManager: Gerencia jobs
   - TaskManagers: 2 instÃ¢ncias com 2 slots cada

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ cmd/                          # AplicaÃ§Ãµes executÃ¡veis
â”‚   â”œâ”€â”€ account_producer/         # Producer de contas
â”‚   â”œâ”€â”€ customer_producer/        # Producer de clientes
â”‚   â”œâ”€â”€ transaction_producer/    # Producer de transaÃ§Ãµes
â”‚   â”œâ”€â”€ transaction_credit_producer/  # Producer de crÃ©ditos
â”‚   â””â”€â”€ transaction_debit_producer/   # Producer de dÃ©bitos
â”‚
â”œâ”€â”€ internal/                     # CÃ³digo interno do projeto
â”‚   â”œâ”€â”€ config/                   # ConfiguraÃ§Ãµes (local/docker)
â”‚   â”œâ”€â”€ handler/                  # Handlers HTTP
â”‚   â”œâ”€â”€ kafka/                    # UtilitÃ¡rios Kafka
â”‚   â”œâ”€â”€ models/                   # Modelos de dados
â”‚   â”œâ”€â”€ router/                   # Roteador HTTP
â”‚   â””â”€â”€ utils/                    # UtilitÃ¡rios gerais
â”‚
â”œâ”€â”€ configs/                      # Arquivos de configuraÃ§Ã£o
â”‚   â”œâ”€â”€ config-local.yaml         # Config para ambiente local
â”‚   â”œâ”€â”€ config-docker.yaml        # Config para ambiente Docker
â”‚   â””â”€â”€ create_transaction_table.flinksql  # SQL do Flink
â”‚
â”œâ”€â”€ docker/                       # ConfiguraÃ§Ãµes Docker
â”‚   â”œâ”€â”€ docker-compose.yaml       # OrquestraÃ§Ã£o de serviÃ§os
â”‚   â”œâ”€â”€ Dockerfile                # Imagem base Flink
â”‚   â”œâ”€â”€ Dockerfile.account        # Imagem do account_producer
â”‚   â””â”€â”€ jars/                     # JARs do Flink
â”‚
â”œâ”€â”€ assets/                       # Recursos estÃ¡ticos
â”‚   â”œâ”€â”€ datasets/                 # Datasets CSV
â”‚   â”‚   â”œâ”€â”€ accounts.csv
â”‚   â”‚   â”œâ”€â”€ customers.csv
â”‚   â”‚   â””â”€â”€ transactions.csv
â”‚   â””â”€â”€ jars/                     # JARs necessÃ¡rios
â”‚       â”œâ”€â”€ flink-connector-kafka-4.0.1-2.0.jar
â”‚       â”œâ”€â”€ flink-connector-jdbc-3.3.0-1.20.jar
â”‚       â”œâ”€â”€ flink-sql-connector-postgres-cdc-3.5.0.jar
â”‚       â””â”€â”€ postgresql-42.7.8.jar
â”‚
â”œâ”€â”€ go.mod                        # MÃ³dulo Go
â”œâ”€â”€ Makefile                      # Comandos automatizados
â”œâ”€â”€ create_topics.sh              # Script para criar tÃ³picos Kafka
â””â”€â”€ README.md                     # Este arquivo
```

## ğŸ”§ Requisitos

- **Go**: 1.24+
- **Docker**: 20.10+
- **Docker Compose**: 2.0+
- **Make**: (opcional, mas recomendado)

## ğŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 1. Clone o repositÃ³rio

```bash
git clone <repository-url>
cd stream-processing-with-apache-flink
```

### 2. Instale as dependÃªncias Go

```bash
go mod download
```

### 3. Configure o ambiente

O projeto suporta dois ambientes:

- **Local**: Para desenvolvimento local
- **Docker**: Para execuÃ§Ã£o em containers

As configuraÃ§Ãµes estÃ£o em `configs/config-local.yaml` e `configs/config-docker.yaml`.

## ğŸƒ Como Executar

### OpÃ§Ã£o 1: Usando Docker Compose (Recomendado)

Inicia todos os serviÃ§os (Kafka, Flink, Producers):

```bash
cd docker
docker-compose up -d
```

Ou usando Make:

```bash
make docker-up
```

**ServiÃ§os disponÃ­veis:**
- Kafka Brokers: `localhost:29092`, `localhost:39092`, `localhost:49092`
- Flink Web UI: http://localhost:8081
- Account Producer: http://localhost:14000

### OpÃ§Ã£o 2: Ambiente Local

#### 1. Inicie o cluster Kafka e Flink

```bash
cd docker
docker-compose up -d kafka1 kafka2 kafka3 jobmanager taskmanager1 taskmanager2
```

#### 2. Crie os tÃ³picos Kafka

```bash
./create_topics.sh
```

Ou manualmente:

```bash
docker exec -it kafka1 kafka-topics --create \
  --bootstrap-server "kafka1:19092,kafka2:19092,kafka3:19092" \
  --replication-factor 3 --partitions 1 \
  --config cleanup.policy=compact --topic accounts
```

#### 3. Execute os producers

```bash
# Compilar todos os executÃ¡veis
make build

# Executar producers individualmente
./bin/account_producer
./bin/customer_producer
./bin/transaction_producer
```

Ou usando Make:

```bash
make run-account-producer
```

## ğŸ“Š TÃ³picos Kafka

| TÃ³pico | Cleanup Policy | DescriÃ§Ã£o |
|--------|---------------|-----------|
| `accounts` | `compact` | Dados de contas bancÃ¡rias |
| `customers` | `compact` | Dados de clientes |
| `transactions` | `delete` | Todas as transaÃ§Ãµes |
| `transactions.credits` | `delete` | TransaÃ§Ãµes de crÃ©dito |
| `transactions.debits` | `delete` | TransaÃ§Ãµes de dÃ©bito |

## ğŸ”Œ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

- `ENVRUN`: Define o ambiente (`local` ou `docker`)

### Arquivos de ConfiguraÃ§Ã£o

- **config-local.yaml**: ConfiguraÃ§Ã£o para ambiente local
  - Brokers: `localhost:29092`, `localhost:39092`, `localhost:49092`

- **config-docker.yaml**: ConfiguraÃ§Ã£o para ambiente Docker
  - Brokers: `kafka1:19092`, `kafka2:19092`, `kafka3:19092`

## ğŸ› ï¸ Comandos Make

O projeto inclui um Makefile com comandos Ãºteis:

```bash
make help              # Mostra todos os comandos disponÃ­veis
make build             # Compila todos os executÃ¡veis
make test              # Executa testes
make test-cover        # Executa testes com cobertura
make fmt               # Formata o cÃ³digo
make vet               # Executa go vet
make lint              # Formata e verifica o cÃ³digo
make tidy              # Organiza dependÃªncias
make clean             # Remove arquivos gerados

# Docker
make docker-up         # Inicia serviÃ§os Docker
make docker-down       # Para serviÃ§os Docker
make docker-logs       # Mostra logs dos serviÃ§os
make docker-ps          # Lista containers em execuÃ§Ã£o
make docker-restart    # Reinicia os serviÃ§os
make docker-clean      # Remove containers e volumes
```

## ğŸ§ª Desenvolvimento

### Estrutura de CÃ³digo

- **cmd/**: AplicaÃ§Ãµes executÃ¡veis principais
- **internal/**: CÃ³digo privado da aplicaÃ§Ã£o
  - `config/`: Gerenciamento de configuraÃ§Ãµes
  - `kafka/`: Cliente e utilitÃ¡rios Kafka
  - `models/`: Modelos de dados e mappers
  - `router/`: Roteamento HTTP
  - `utils/`: UtilitÃ¡rios gerais

### Adicionar um Novo Producer

1. Crie um novo diretÃ³rio em `cmd/`:
```bash
mkdir -p cmd/my_producer
```

2. Crie `main.go` seguindo o padrÃ£o dos outros producers

3. Adicione ao Makefile se necessÃ¡rio

4. Compile: `make build`

### Executar Testes

```bash
# Todos os testes
make test

# Testes com cobertura
make test-cover

# Testes rÃ¡pidos
make test-short
```

## ğŸ³ Docker

### Estrutura Docker

- **Dockerfile**: Imagem base do Flink com conectores
- **Dockerfile.account**: Imagem do account_producer
- **docker-compose.yaml**: OrquestraÃ§Ã£o completa

### Build de Imagens

```bash
# Build da imagem Flink
docker build -f docker/Dockerfile -t flink-custom .

# Build do account_producer
docker build -f docker/Dockerfile.account -t account-producer .
```

### Logs

```bash
# Todos os logs
make docker-logs

# Logs de um serviÃ§o especÃ­fico
docker-compose -f docker/docker-compose.yaml logs -f account-producer
```

## ğŸ“ˆ Monitoramento

### Flink Web UI

Acesse http://localhost:8081 para:
- Visualizar jobs em execuÃ§Ã£o
- Monitorar performance
- Verificar logs
- Submeter novos jobs SQL

### Kafka

Use ferramentas como:
- **kafka-console-consumer**: Para consumir mensagens
- **kafka-topics**: Para gerenciar tÃ³picos
- **AKHQ** (opcional): Interface web para Kafka

Exemplo de consumo:

```bash
docker exec -it kafka1 kafka-console-consumer \
  --bootstrap-server kafka1:19092 \
  --topic accounts \
  --from-beginning
```

## ğŸ” Modelos de Dados

### Account
```go
type Account struct {
    AccountID    string
    DistrictID   string
    Frequency    string
    CreationDate time.Time
    UpdateTime   time.Time
}
```

### Customer
```go
type Customer struct {
    CustomerID string
    Sex        string
    Social     string
    FullName   string
    Phone      string
    Email      string
    Address1   string
    Address2   string
    City       string
    State      string
    Zipcode    string
    DistrictID string
    BirthDate  time.Time
    UpdateTime time.Time
}
```

### Transaction
```go
type Transaction struct {
    TransactionID string
    AccountID     string
    Type          string
    Operation     string
    Amount        float64
    Balance       float64
    KSymbol       string
    EventTime     time.Time
    CustomerID    string
}
```

## ğŸš¨ Troubleshooting

### Kafka nÃ£o estÃ¡ acessÃ­vel

Verifique se os containers estÃ£o rodando:
```bash
docker-compose -f docker/docker-compose.yaml ps
```

### TÃ³picos nÃ£o existem

Crie os tÃ³picos manualmente:
```bash
./create_topics.sh
```

### Producer nÃ£o consegue conectar

Verifique:
1. VariÃ¡vel `ENVRUN` estÃ¡ configurada corretamente
2. Arquivo de configuraÃ§Ã£o existe em `configs/`
3. Brokers estÃ£o acessÃ­veis

### Flink nÃ£o processa dados

1. Verifique se os JARs estÃ£o em `docker/jars/`
2. Confirme que os tÃ³picos existem
3. Verifique os logs do Flink: `docker-compose logs jobmanager`

## ğŸ“ LicenÃ§a

MIT

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“š Recursos Adicionais

- [Apache Flink Documentation](https://flink.apache.org/docs/)
- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [Go Kafka Client](https://github.com/confluentinc/confluent-kafka-go)

---

**Desenvolvido com â¤ï¸ usando Go, Apache Flink e Apache Kafka**
